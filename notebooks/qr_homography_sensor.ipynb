{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from queue import PriorityQueue\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_processing_power(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measure CPU and memory usage during the execution of a function.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function to measure.\n",
    "        *args: Positional arguments for the function.\n",
    "        **kwargs: Keyword arguments for the function.\n",
    "\n",
    "    Returns:\n",
    "        result: The result of the function execution.\n",
    "        stats: A dictionary containing CPU and memory usage stats.\n",
    "    \"\"\"\n",
    "    # Record initial stats\n",
    "    process = psutil.Process()\n",
    "    start_cpu = process.cpu_percent(interval=None)\n",
    "    start_mem = process.memory_info().rss\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Run the function\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    # Record final stats\n",
    "    end_cpu = process.cpu_percent(interval=None)\n",
    "    end_mem = process.memory_info().rss\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # Calculate deltas\n",
    "    cpu_usage = end_cpu - start_cpu\n",
    "    mem_usage = end_mem - start_mem\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    stats = {\n",
    "        \"CPU_Usage (%)\": cpu_usage,\n",
    "        \"Memory_Usage (bytes)\": mem_usage,\n",
    "        \"Duration (seconds)\": duration\n",
    "    }\n",
    "\n",
    "    return result, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(image, detections, label_map=None, color=(0, 255, 0), thickness=5):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on an image to visualize detections.\n",
    "\n",
    "    Args:\n",
    "        image (ndarray): The image on which to draw detections.\n",
    "        detections (list): List of detections. Each detection is a dictionary containing:\n",
    "            - 'x_min': Minimum x-coordinate of the bounding box.\n",
    "            - 'y_min': Minimum y-coordinate of the bounding box.\n",
    "            - 'x_max': Maximum x-coordinate of the bounding box.\n",
    "            - 'y_max': Maximum y-coordinate of the bounding box.\n",
    "            - 'label' (optional): Label or class ID of the detection.\n",
    "        label_map (dict, optional): A mapping from class IDs to human-readable labels.\n",
    "        color (tuple): Color of the bounding box (default: green).\n",
    "        thickness (int): Thickness of the bounding box lines (default: 2).\n",
    "\n",
    "    Returns:\n",
    "        annotated_image (ndarray): The image with drawn detections.\n",
    "    \"\"\"\n",
    "    annotated_image = image.copy()\n",
    "\n",
    "    for detection in detections:\n",
    "        x_min = int(detection['x_min'])\n",
    "        y_min = int(detection['y_min'])\n",
    "        x_max = int(detection['x_max'])\n",
    "        y_max = int(detection['y_max'])\n",
    "        label = detection.get('label', None)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(annotated_image, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "\n",
    "        # Draw the label if available\n",
    "        if label is not None:\n",
    "            label_text = str(label_map[label]) if label_map and label in label_map else str(label)\n",
    "            cv2.putText(\n",
    "                annotated_image,\n",
    "                label_text,\n",
    "                (x_min, y_min - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                2,\n",
    "                color,\n",
    "                5\n",
    "            )\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "def save_yolo_labels( labels, output_dir, image_name ):\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    label_path = f\"{output_dir}/labels/{image_name}.txt\"\n",
    "    with open(label_path, \"w\") as f:\n",
    "        for label in labels:\n",
    "            f.write(\" \".join(map(str, label)) + \"\\n\")\n",
    "    print(f\"Saved YOLO label file: {label_path}\")\n",
    "\n",
    "def save_annotated_image( image, detections, output_dir, image_name, suffix=\"\" ):\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    annotated_image = draw_detections(image, detections)\n",
    "    annotated_image_path = f\"{output_dir}/images/{image_name}_result{suffix}.jpg\"\n",
    "    cv2.imwrite(annotated_image_path, annotated_image)\n",
    "    print(f\"Saved annotated image: {annotated_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_code_detector(image,):\n",
    "    \"\"\"\n",
    "    Detect QR codes using OpenCV and save results in YOLO format.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        output_dir (str): Directory to save results (images and labels).\n",
    "        save_annotated (bool): If True, save the annotated image with QR code detections.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Initialize QR Code detector\n",
    "    qr_detector = cv2.QRCodeDetector()\n",
    "\n",
    "    # Detect and decode QR codes\n",
    "    data, _, vertices, _ = qr_detector.detectAndDecodeMulti(image)\n",
    "\n",
    "    detections = []  # Store detection information for drawing\n",
    "    labels = []      # Store YOLO-format labels\n",
    "\n",
    "    if vertices is not None:\n",
    "        for i, points in enumerate(vertices):\n",
    "            # Calculate bounding box coordinates\n",
    "            x_min = points[:, 0].min()\n",
    "            y_min = points[:, 1].min()\n",
    "            x_max = points[:, 0].max()\n",
    "            y_max = points[:, 1].max()\n",
    "\n",
    "            # Append to detections for visualization\n",
    "            detections.append({\n",
    "                'x_min': x_min,\n",
    "                'y_min': y_min,\n",
    "                'x_max': x_max,\n",
    "                'y_max': y_max,\n",
    "                'label': f\"QR {i}\"  # Optional label for visualization\n",
    "            })\n",
    "\n",
    "            # Calculate YOLO-format labels\n",
    "            x_center = (x_min + x_max) / 2 / w  # Normalize by image width\n",
    "            y_center = (y_min + y_max) / 2 / h  # Normalize by image height\n",
    "            width = (x_max - x_min) / w         # Normalize width\n",
    "            height = (y_max - y_min) / h        # Normalize height\n",
    "\n",
    "            labels.append([0, x_center, y_center, width, height])  # Assuming class_id = 0 for QR codes\n",
    "    else:\n",
    "        print(f\"No QR codes detected in the image\")\n",
    "\n",
    "    return detections, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/test/samples/test_light_130.JPG\n",
      "[{'x_min': 1908.8589, 'y_min': 3035.5122, 'x_max': 1994.0, 'y_max': 3157.0, 'label': 'QR 0'}, {'x_min': 1903.0, 'y_min': 2585.0, 'x_max': 1992.0, 'y_max': 2708.0, 'label': 'QR 1'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_130.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_130_result.jpg\n",
      "Processing: ../data/test/samples/test_light_120.JPG\n",
      "[{'x_min': 1776.0, 'y_min': 2511.7073, 'x_max': 1873.1279, 'y_max': 2644.0, 'label': 'QR 0'}, {'x_min': 2317.0, 'y_min': 3028.6, 'x_max': 2440.049, 'y_max': 3183.338, 'label': 'QR 1'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_120.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_120_result.jpg\n",
      "Processing: ../data/test/samples/test_110.JPG\n",
      "[{'x_min': 2557.0, 'y_min': 2240.0, 'x_max': 2761.0, 'y_max': 2484.0, 'label': 'QR 0'}, {'x_min': 1717.1102, 'y_min': 3058.7502, 'x_max': 1861.0, 'y_max': 3263.0, 'label': 'QR 1'}, {'x_min': 1717.0493, 'y_min': 2304.5298, 'x_max': 1866.1903, 'y_max': 2514.4285, 'label': 'QR 2'}, {'x_min': 2557.0, 'y_min': 3096.0, 'x_max': 2746.3745, 'y_max': 3333.7847, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_110.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_110_result.jpg\n",
      "Processing: ../data/test/samples/test_60.JPG\n",
      "[{'x_min': 2404.0, 'y_min': 990.0, 'x_max': 2807.0, 'y_max': 1503.0, 'label': 'QR 0'}, {'x_min': 962.63916, 'y_min': 2605.0, 'x_max': 1189.0, 'y_max': 2950.5154, 'label': 'QR 1'}, {'x_min': 2389.839, 'y_min': 2655.601, 'x_max': 2745.0, 'y_max': 3098.7688, 'label': 'QR 2'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_60.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_60_result.jpg\n",
      "Processing: ../data/test/samples/test_light_60.JPG\n",
      "[{'x_min': 1414.6531, 'y_min': 3227.298, 'x_max': 1578.7433, 'y_max': 3486.474, 'label': 'QR 0'}, {'x_min': 1422.0, 'y_min': 2313.3013, 'x_max': 1592.0825, 'y_max': 2570.0, 'label': 'QR 1'}, {'x_min': 2436.0, 'y_min': 3341.0, 'x_max': 2693.9868, 'y_max': 3678.6663, 'label': 'QR 2'}, {'x_min': 2442.648, 'y_min': 2199.8103, 'x_max': 2726.1829, 'y_max': 2533.1138, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_60.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_60_result.jpg\n",
      "Processing: ../data/test/samples/test_light_140.JPG\n",
      "[{'x_min': 2434.0, 'y_min': 2914.0, 'x_max': 2543.0, 'y_max': 3045.0, 'label': 'QR 0'}, {'x_min': 2435.0, 'y_min': 2451.0, 'x_max': 2546.0, 'y_max': 2580.0, 'label': 'QR 1'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_140.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_140_result.jpg\n",
      "Processing: ../data/test/samples/test_marks.JPG\n",
      "[{'x_min': 2364.7778, 'y_min': 1623.0138, 'x_max': 2547.1968, 'y_max': 1855.8821, 'label': 'QR 0'}, {'x_min': 3317.0, 'y_min': 2600.667, 'x_max': 3557.0, 'y_max': 2894.2947, 'label': 'QR 1'}, {'x_min': 3360.4148, 'y_min': 1623.0, 'x_max': 3615.5322, 'y_max': 1899.5979, 'label': 'QR 2'}, {'x_min': 2328.4646, 'y_min': 2476.3237, 'x_max': 2507.452, 'y_max': 2722.0, 'label': 'QR 3'}, {'x_min': 1993.0, 'y_min': 21.0, 'x_max': 5709.0, 'y_max': 1544.0, 'label': 'QR 4'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_marks.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_marks_result.jpg\n",
      "Processing: ../data/test/samples/test_light_70.JPG\n",
      "[{'x_min': 2397.0, 'y_min': 2430.8281, 'x_max': 2625.1262, 'y_max': 2693.0, 'label': 'QR 0'}, {'x_min': 1533.0, 'y_min': 2491.0, 'x_max': 1682.0, 'y_max': 2703.0, 'label': 'QR 1'}, {'x_min': 1519.958, 'y_min': 3262.205, 'x_max': 1664.7828, 'y_max': 3482.7305, 'label': 'QR 2'}, {'x_min': 2388.365, 'y_min': 3357.0, 'x_max': 2597.9822, 'y_max': 3632.9995, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_70.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_70_result.jpg\n",
      "Processing: ../data/test/samples/test_70.JPG\n",
      "[{'x_min': 1415.7003, 'y_min': 2816.0, 'x_max': 1617.0, 'y_max': 3125.0, 'label': 'QR 0'}, {'x_min': 1394.0, 'y_min': 1639.8494, 'x_max': 1608.0846, 'y_max': 1982.2845, 'label': 'QR 1'}, {'x_min': 2687.0, 'y_min': 2867.0, 'x_max': 3007.6165, 'y_max': 3261.6165, 'label': 'QR 2'}, {'x_min': 2684.0, 'y_min': 1410.3877, 'x_max': 3033.2861, 'y_max': 1854.0, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_70.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_70_result.jpg\n",
      "Processing: ../data/test/samples/test_100.JPG\n",
      "[{'x_min': 1663.759, 'y_min': 2964.667, 'x_max': 1820.0, 'y_max': 3189.8425, 'label': 'QR 0'}, {'x_min': 2596.0, 'y_min': 2022.638, 'x_max': 2828.4312, 'y_max': 2307.0, 'label': 'QR 1'}, {'x_min': 1659.0, 'y_min': 2119.689, 'x_max': 1822.1226, 'y_max': 2358.0, 'label': 'QR 2'}, {'x_min': 2596.0, 'y_min': 2999.0, 'x_max': 2811.1887, 'y_max': 3265.837, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_100.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_100_result.jpg\n",
      "Processing: ../data/test/samples/test_light_80.JPG\n",
      "[{'x_min': 1632.1788, 'y_min': 3150.1353, 'x_max': 1763.921, 'y_max': 3345.4001, 'label': 'QR 0'}, {'x_min': 2416.0, 'y_min': 2432.3975, 'x_max': 2616.0732, 'y_max': 2658.0, 'label': 'QR 1'}, {'x_min': 1649.0, 'y_min': 2468.4111, 'x_max': 1785.0739, 'y_max': 2655.0, 'label': 'QR 2'}, {'x_min': 2400.0283, 'y_min': 3235.0, 'x_max': 2582.0, 'y_max': 3472.572, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_80.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_80_result.jpg\n",
      "Processing: ../data/test/samples/test_130.JPG\n",
      "[{'x_min': 1793.7345, 'y_min': 3074.1233, 'x_max': 1919.0, 'y_max': 3250.6067, 'label': 'QR 0'}, {'x_min': 2515.0, 'y_min': 2401.0, 'x_max': 2689.0, 'y_max': 2601.0, 'label': 'QR 1'}, {'x_min': 1800.0, 'y_min': 2430.0, 'x_max': 1930.0, 'y_max': 2606.0327, 'label': 'QR 2'}, {'x_min': 2509.4224, 'y_min': 3118.0, 'x_max': 2666.9966, 'y_max': 3319.7437, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_130.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_130_result.jpg\n",
      "Processing: ../data/test/samples/test_80.JPG\n",
      "[{'x_min': 1502.6378, 'y_min': 3149.0, 'x_max': 1688.0, 'y_max': 3424.389, 'label': 'QR 0'}, {'x_min': 1496.0, 'y_min': 2127.3955, 'x_max': 1689.1326, 'y_max': 2416.0, 'label': 'QR 1'}, {'x_min': 2636.0, 'y_min': 3202.0, 'x_max': 2912.022, 'y_max': 3540.4526, 'label': 'QR 2'}, {'x_min': 2631.0, 'y_min': 1982.8735, 'x_max': 2921.2644, 'y_max': 2340.0, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_80.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_80_result.jpg\n",
      "Processing: ../data/test/samples/test_120.JPG\n",
      "[{'x_min': 2471.0, 'y_min': 2369.6733, 'x_max': 2651.3357, 'y_max': 2587.0, 'label': 'QR 0'}, {'x_min': 1712.2913, 'y_min': 3102.819, 'x_max': 1843.0077, 'y_max': 3288.3667, 'label': 'QR 1'}, {'x_min': 1712.0, 'y_min': 2421.0, 'x_max': 1847.0, 'y_max': 2609.0, 'label': 'QR 2'}, {'x_min': 2471.0, 'y_min': 3134.0, 'x_max': 2640.0376, 'y_max': 3346.1855, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_120.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_120_result.jpg\n",
      "Processing: ../data/test/samples/test_90.JPG\n",
      "[{'x_min': 1587.0, 'y_min': 2251.39, 'x_max': 1765.1676, 'y_max': 2500.0, 'label': 'QR 0'}, {'x_min': 1576.1646, 'y_min': 3147.7441, 'x_max': 1746.0, 'y_max': 3395.4216, 'label': 'QR 1'}, {'x_min': 2577.4863, 'y_min': 3220.0, 'x_max': 2810.994, 'y_max': 3517.8765, 'label': 'QR 2'}, {'x_min': 2588.0, 'y_min': 2174.692, 'x_max': 2842.3918, 'y_max': 2473.0, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_90.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_90_result.jpg\n",
      "Processing: ../data/test/samples/test_light_90.JPG\n",
      "[{'x_min': 1604.1815, 'y_min': 3114.5056, 'x_max': 1725.0, 'y_max': 3289.0361, 'label': 'QR 0'}, {'x_min': 2314.0, 'y_min': 2467.438, 'x_max': 2491.1755, 'y_max': 2668.0, 'label': 'QR 1'}, {'x_min': 1617.0, 'y_min': 2495.0, 'x_max': 1741.0, 'y_max': 2663.0, 'label': 'QR 2'}, {'x_min': 2304.0, 'y_min': 3182.0, 'x_max': 2463.0, 'y_max': 3389.0144, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_90.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_90_result.jpg\n",
      "Processing: ../data/test/samples/test_light_110.JPG\n",
      "[{'x_min': 1680.013, 'y_min': 3027.7073, 'x_max': 1782.0, 'y_max': 3173.0, 'label': 'QR 0'}, {'x_min': 1680.0, 'y_min': 2496.6758, 'x_max': 1787.0767, 'y_max': 2642.0, 'label': 'QR 1'}, {'x_min': 2270.0, 'y_min': 3068.775, 'x_max': 2402.9758, 'y_max': 3238.0696, 'label': 'QR 2'}, {'x_min': 2273.0, 'y_min': 2470.4158, 'x_max': 2416.7473, 'y_max': 2639.0, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_110.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_110_result.jpg\n",
      "Processing: ../data/test/samples/test_140.JPG\n",
      "[{'x_min': 2321.0, 'y_min': 3123.0, 'x_max': 2467.0522, 'y_max': 3305.885, 'label': 'QR 0'}, {'x_min': 1654.9133, 'y_min': 3094.0, 'x_max': 1771.0, 'y_max': 3256.323, 'label': 'QR 1'}, {'x_min': 2320.0, 'y_min': 2460.5781, 'x_max': 2477.0781, 'y_max': 2647.1938, 'label': 'QR 2'}, {'x_min': 1654.0176, 'y_min': 2491.0854, 'x_max': 1775.2887, 'y_max': 2658.0356, 'label': 'QR 3'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_140.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_140_result.jpg\n",
      "Processing: ../data/test/samples/test_light_100.JPG\n",
      "[{'x_min': 1673.0498, 'y_min': 3069.5454, 'x_max': 1784.7743, 'y_max': 3227.3557, 'label': 'QR 0'}, {'x_min': 2320.0, 'y_min': 2486.2295, 'x_max': 2476.5928, 'y_max': 2666.2295, 'label': 'QR 1'}, {'x_min': 1685.0, 'y_min': 2504.9326, 'x_max': 1799.4225, 'y_max': 2660.0535, 'label': 'QR 2'}]\n",
      "Saved YOLO label file: ../data/test/results/cv2_detector//labels/test_light_100.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_100_result.jpg\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "test_dir = \"../data/test/samples/\"\n",
    "cv2_results_dir = \"../data/test/results/cv2_detector/\"\n",
    "\n",
    "# Run QR code detection and save results for each image in the test directory\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename.endswith((\".jpg\", \".png\", \".jpeg\", \".JPG\")) and \"out\" not in filename and filename[0] != \".\":\n",
    "        filepath = os.path.join(test_dir, filename)\n",
    "        image_name = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        print(f\"Processing: {filepath}\")\n",
    "        image = cv2.imread(filepath)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Error: Could not load image at {filepath}\")\n",
    "        cv2_detections, cv2_labels = qr_code_detector(image)\n",
    "        print(cv2_detections)\n",
    "        save_yolo_labels(cv2_labels, cv2_results_dir, image_name)\n",
    "        save_annotated_image( image, cv2_detections, cv2_results_dir, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolov5n_detector(image, model):\n",
    "    \"\"\"\n",
    "    Perform inference with YOLOv5 and save the results as annotated images and YOLO labels.\n",
    "\n",
    "    Args:\n",
    "        model: Loaded YOLOv5 model.\n",
    "        image_path (str): Path to the input image.\n",
    "        output_dir (str): Directory to save results (images and labels).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Load the image\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(image)\n",
    "\n",
    "    # Process detections\n",
    "    detections = []\n",
    "    labels = []\n",
    "    for _, row in results.pandas().xyxy[0].iterrows():\n",
    "        x_min = row['xmin']\n",
    "        y_min = row['ymin']\n",
    "        x_max = row['xmax']\n",
    "        y_max = row['ymax']\n",
    "        class_id = int(row['class'])\n",
    "\n",
    "        detections.append({\n",
    "            'x_min': x_min,\n",
    "            'y_min': y_min,\n",
    "            'x_max': x_max,\n",
    "            'y_max': y_max,\n",
    "            'label': class_id\n",
    "        })\n",
    "\n",
    "        # Normalize for YOLO format\n",
    "        x_center = (x_min + x_max) / 2 / w\n",
    "        y_center = (y_min + y_max) / 2 / h\n",
    "        width = (x_max - x_min) / w\n",
    "        height = (y_max - y_min) / h\n",
    "\n",
    "        labels.append([class_id, x_center, y_center, width, height])\n",
    "\n",
    "    return detections, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2024-12-12 Python-3.11.11 torch-2.5.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1760518 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n",
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Processing: ../data/test/samples/test_light_130.JPG\n",
      "[{'x_min': 2417.516845703125, 'y_min': 2572.69189453125, 'x_max': 2528.23388671875, 'y_max': 2691.4248046875, 'label': 0}, {'x_min': 2428.328857421875, 'y_min': 3100.721435546875, 'x_max': 2520.1484375, 'y_max': 3200.66796875, 'label': 0}, {'x_min': 1914.5042724609375, 'y_min': 3091.47900390625, 'x_max': 1985.30615234375, 'y_max': 3157.51513671875, 'label': 0}, {'x_min': 1895.886474609375, 'y_min': 2585.45458984375, 'x_max': 1973.0841064453125, 'y_max': 2673.521484375, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_130.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_130_result.jpg\n",
      "Processing: ../data/test/samples/test_light_120.JPG\n",
      "[{'x_min': 2333.33984375, 'y_min': 3054.1123046875, 'x_max': 2438.814697265625, 'y_max': 3178.10498046875, 'label': 0}, {'x_min': 2335.583984375, 'y_min': 2499.048828125, 'x_max': 2452.3134765625, 'y_max': 2626.779541015625, 'label': 0}, {'x_min': 1778.3275146484375, 'y_min': 2516.528564453125, 'x_max': 1864.3179931640625, 'y_max': 2617.95361328125, 'label': 0}, {'x_min': 1780.0947265625, 'y_min': 3039.491455078125, 'x_max': 1852.034912109375, 'y_max': 3125.603271484375, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_120.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_120_result.jpg\n",
      "Processing: ../data/test/samples/test_110.JPG\n",
      "[{'x_min': 2564.634765625, 'y_min': 2241.718994140625, 'x_max': 2769.63037109375, 'y_max': 2467.222900390625, 'label': 0}, {'x_min': 1712.2957763671875, 'y_min': 2307.879638671875, 'x_max': 1864.3236083984375, 'y_max': 2499.845703125, 'label': 0}, {'x_min': 2563.293701171875, 'y_min': 3120.527587890625, 'x_max': 2752.515625, 'y_max': 3317.4228515625, 'label': 0}, {'x_min': 1718.5660400390625, 'y_min': 3074.98779296875, 'x_max': 1855.0997314453125, 'y_max': 3262.615966796875, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_110.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_110_result.jpg\n",
      "Processing: ../data/test/samples/test_60.JPG\n",
      "[{'x_min': 947.5664672851562, 'y_min': 2638.19873046875, 'x_max': 1199.2791748046875, 'y_max': 2933.15673828125, 'label': 0}, {'x_min': 925.281005859375, 'y_min': 1292.0413818359375, 'x_max': 1190.1846923828125, 'y_max': 1624.69970703125, 'label': 0}, {'x_min': 2392.1064453125, 'y_min': 2981.867919921875, 'x_max': 2467.295654296875, 'y_max': 3065.427001953125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_60.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_60_result.jpg\n",
      "Processing: ../data/test/samples/test_light_60.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2442.983642578125, 'y_min': 2209.456787109375, 'x_max': 2741.592041015625, 'y_max': 2517.808349609375, 'label': 0}, {'x_min': 1417.423828125, 'y_min': 2317.907958984375, 'x_max': 1592.842529296875, 'y_max': 2564.71142578125, 'label': 0}, {'x_min': 2432.8779296875, 'y_min': 3366.525146484375, 'x_max': 2709.56591796875, 'y_max': 3671.94970703125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_60.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_60_result.jpg\n",
      "Processing: ../data/test/samples/test_light_140.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2447.86181640625, 'y_min': 2940.663818359375, 'x_max': 2534.24755859375, 'y_max': 3035.812744140625, 'label': 0}, {'x_min': 1963.061767578125, 'y_min': 2933.912841796875, 'x_max': 2023.806640625, 'y_max': 3000.37109375, 'label': 0}, {'x_min': 1961.6697998046875, 'y_min': 2475.522216796875, 'x_max': 2027.741943359375, 'y_max': 2553.349609375, 'label': 0}, {'x_min': 2447.7060546875, 'y_min': 2455.333984375, 'x_max': 2539.146484375, 'y_max': 2556.357421875, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_140.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_140_result.jpg\n",
      "Processing: ../data/test/samples/test_marks.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2359.702880859375, 'y_min': 1640.8089599609375, 'x_max': 2539.14208984375, 'y_max': 1848.1875, 'label': 0}, {'x_min': 3331.3447265625, 'y_min': 2628.21728515625, 'x_max': 3570.2890625, 'y_max': 2875.64892578125, 'label': 0}, {'x_min': 3366.26708984375, 'y_min': 1608.8724365234375, 'x_max': 3625.2529296875, 'y_max': 1882.267822265625, 'label': 0}, {'x_min': 2331.0126953125, 'y_min': 2488.594482421875, 'x_max': 2504.445556640625, 'y_max': 2715.67529296875, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_marks.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_marks_result.jpg\n",
      "Processing: ../data/test/samples/test_light_70.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2405.330322265625, 'y_min': 2424.170654296875, 'x_max': 2638.28125, 'y_max': 2682.642578125, 'label': 0}, {'x_min': 1526.0128173828125, 'y_min': 2489.657958984375, 'x_max': 1679.6871337890625, 'y_max': 2686.602294921875, 'label': 0}, {'x_min': 2406.89697265625, 'y_min': 3379.841796875, 'x_max': 2605.318603515625, 'y_max': 3623.137451171875, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_70.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_70_result.jpg\n",
      "Processing: ../data/test/samples/test_70.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 1374.4727783203125, 'y_min': 1648.92333984375, 'x_max': 1620.64892578125, 'y_max': 1957.55078125, 'label': 0}, {'x_min': 1412.7169189453125, 'y_min': 2836.740966796875, 'x_max': 1624.9501953125, 'y_max': 3127.637939453125, 'label': 0}, {'x_min': 2688.716796875, 'y_min': 2891.553466796875, 'x_max': 3031.14111328125, 'y_max': 3259.91943359375, 'label': 0}, {'x_min': 2693.231689453125, 'y_min': 1451.5548095703125, 'x_max': 3057.015869140625, 'y_max': 1829.7159423828125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_70.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_70_result.jpg\n",
      "Processing: ../data/test/samples/test_100.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2594.42138671875, 'y_min': 2013.4345703125, 'x_max': 2841.83251953125, 'y_max': 2284.75439453125, 'label': 0}, {'x_min': 1661.197265625, 'y_min': 2982.586181640625, 'x_max': 1819.8739013671875, 'y_max': 3183.565185546875, 'label': 0}, {'x_min': 1650.5001220703125, 'y_min': 2127.881103515625, 'x_max': 1815.5770263671875, 'y_max': 2343.120361328125, 'label': 0}, {'x_min': 2609.933349609375, 'y_min': 3002.185791015625, 'x_max': 2813.69677734375, 'y_max': 3237.53466796875, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_100.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_100_result.jpg\n",
      "Processing: ../data/test/samples/test_light_80.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2414.628173828125, 'y_min': 3265.69580078125, 'x_max': 2585.26806640625, 'y_max': 3455.589599609375, 'label': 0}, {'x_min': 1644.4853515625, 'y_min': 2465.12109375, 'x_max': 1777.304931640625, 'y_max': 2624.016357421875, 'label': 0}, {'x_min': 2430.856201171875, 'y_min': 2434.9970703125, 'x_max': 2613.96728515625, 'y_max': 2632.015380859375, 'label': 0}, {'x_min': 1634.1802978515625, 'y_min': 3171.18408203125, 'x_max': 1745.0264892578125, 'y_max': 3329.06689453125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_80.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_80_result.jpg\n",
      "Processing: ../data/test/samples/test_130.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 1795.9844970703125, 'y_min': 2439.292724609375, 'x_max': 1926.986572265625, 'y_max': 2595.4541015625, 'label': 0}, {'x_min': 1789.6585693359375, 'y_min': 3107.40380859375, 'x_max': 1910.4364013671875, 'y_max': 3250.906494140625, 'label': 0}, {'x_min': 2519.730712890625, 'y_min': 2403.61865234375, 'x_max': 2690.654541015625, 'y_max': 2587.577392578125, 'label': 0}, {'x_min': 2521.348388671875, 'y_min': 3131.705322265625, 'x_max': 2670.6552734375, 'y_max': 3301.17578125, 'label': 0}, {'x_min': 33.236412048339844, 'y_min': 837.655029296875, 'x_max': 95.30988311767578, 'y_max': 906.0447998046875, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_130.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_130_result.jpg\n",
      "Processing: ../data/test/samples/test_80.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2627.249267578125, 'y_min': 1979.8544921875, 'x_max': 2941.024658203125, 'y_max': 2315.9912109375, 'label': 0}, {'x_min': 1484.45263671875, 'y_min': 2127.05029296875, 'x_max': 1698.8109130859375, 'y_max': 2399.389404296875, 'label': 0}, {'x_min': 2634.2080078125, 'y_min': 3219.441162109375, 'x_max': 2930.100341796875, 'y_max': 3522.52294921875, 'label': 0}, {'x_min': 1491.980224609375, 'y_min': 3160.3427734375, 'x_max': 1685.6278076171875, 'y_max': 3425.112548828125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_80.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_80_result.jpg\n",
      "Processing: ../data/test/samples/test_120.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2481.75830078125, 'y_min': 2363.67431640625, 'x_max': 2657.0927734375, 'y_max': 2560.60546875, 'label': 0}, {'x_min': 1711.778076171875, 'y_min': 2425.810302734375, 'x_max': 1849.0482177734375, 'y_max': 2594.48095703125, 'label': 0}, {'x_min': 4052.83447265625, 'y_min': 4119.7919921875, 'x_max': 4136.87158203125, 'y_max': 4213.55029296875, 'label': 0}, {'x_min': 2509.5986328125, 'y_min': 3150.450927734375, 'x_max': 2641.567138671875, 'y_max': 3287.374755859375, 'label': 0}, {'x_min': 1712.3138427734375, 'y_min': 3142.631591796875, 'x_max': 1832.6044921875, 'y_max': 3289.33642578125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_120.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_120_result.jpg\n",
      "Processing: ../data/test/samples/test_90.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 2579.187744140625, 'y_min': 2163.5712890625, 'x_max': 2860.412353515625, 'y_max': 2456.51123046875, 'label': 0}, {'x_min': 2581.36083984375, 'y_min': 3222.9912109375, 'x_max': 2832.16845703125, 'y_max': 3513.106201171875, 'label': 0}, {'x_min': 1585.2679443359375, 'y_min': 2252.513671875, 'x_max': 1766.136474609375, 'y_max': 2494.252197265625, 'label': 0}, {'x_min': 1575.704833984375, 'y_min': 3171.635986328125, 'x_max': 1742.20751953125, 'y_max': 3396.885986328125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_90.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_90_result.jpg\n",
      "Processing: ../data/test/samples/test_light_90.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 1611.850830078125, 'y_min': 2491.471435546875, 'x_max': 1726.334716796875, 'y_max': 2630.08837890625, 'label': 0}, {'x_min': 2311.16552734375, 'y_min': 3212.04052734375, 'x_max': 2469.973876953125, 'y_max': 3386.016357421875, 'label': 0}, {'x_min': 2324.437255859375, 'y_min': 2476.5537109375, 'x_max': 2491.785400390625, 'y_max': 2659.546142578125, 'label': 0}, {'x_min': 1604.3231201171875, 'y_min': 3150.510498046875, 'x_max': 1712.5218505859375, 'y_max': 3284.84521484375, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_90.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_90_result.jpg\n",
      "Processing: ../data/test/samples/test_light_110.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 1678.4371337890625, 'y_min': 2502.092041015625, 'x_max': 1778.25341796875, 'y_max': 2622.346923828125, 'label': 0}, {'x_min': 2289.668212890625, 'y_min': 3117.958740234375, 'x_max': 2402.458740234375, 'y_max': 3236.90966796875, 'label': 0}, {'x_min': 2281.39306640625, 'y_min': 2479.793212890625, 'x_max': 2406.967529296875, 'y_max': 2623.405517578125, 'label': 0}, {'x_min': 1687.268310546875, 'y_min': 3087.055908203125, 'x_max': 1758.6217041015625, 'y_max': 3171.270751953125, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_110.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_110_result.jpg\n",
      "Processing: ../data/test/samples/test_140.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x_min': 1658.0960693359375, 'y_min': 3112.94580078125, 'x_max': 1775.2559814453125, 'y_max': 3255.72021484375, 'label': 0}, {'x_min': 2324.142578125, 'y_min': 2469.575439453125, 'x_max': 2474.67578125, 'y_max': 2639.33447265625, 'label': 0}, {'x_min': 1651.5953369140625, 'y_min': 2501.152587890625, 'x_max': 1771.2982177734375, 'y_max': 2631.46826171875, 'label': 0}, {'x_min': 4121.7509765625, 'y_min': 3149.30810546875, 'x_max': 4202.1142578125, 'y_max': 3221.99365234375, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_140.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_140_result.jpg\n",
      "Processing: ../data/test/samples/test_light_100.JPG\n",
      "[{'x_min': 2324.877685546875, 'y_min': 2490.340087890625, 'x_max': 2479.45166015625, 'y_max': 2659.4775390625, 'label': 0}, {'x_min': 1683.723876953125, 'y_min': 2501.897705078125, 'x_max': 1787.30322265625, 'y_max': 2609.648193359375, 'label': 0}, {'x_min': 1677.6610107421875, 'y_min': 3114.449951171875, 'x_max': 1772.2164306640625, 'y_max': 3228.260986328125, 'label': 0}, {'x_min': 2328.163330078125, 'y_min': 3174.552001953125, 'x_max': 2452.449951171875, 'y_max': 3306.395263671875, 'label': 0}]\n",
      "Saved YOLO label file: ../data/test/results/yolov5n//labels/test_light_100.txt\n",
      "Saved annotated image: ../data/test/results/cv2_detector//images/test_light_100_result.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='../models/yolov5n_best.pt')\n",
    "\n",
    "# Verify the model is loaded\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Input and output paths\n",
    "test_dir = \"../data/test/samples/\"\n",
    "yolo_results_dir = \"../data/test/results/yolov5n/\"\n",
    "\n",
    "# Run QR code detection and save results for each image in the test directory\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename.endswith((\".jpg\", \".png\", \".jpeg\", \".JPG\")) and \"out\" not in filename and filename[0] != \".\":\n",
    "        filepath = os.path.join(test_dir, filename)\n",
    "        image_name = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        print(f\"Processing: {filepath}\")\n",
    "        image = cv2.imread(filepath)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Error: Could not load image at {filepath}\")\n",
    "        yolo_detections, yolo_labels = yolov5n_detector(image, model)\n",
    "        print(yolo_detections)\n",
    "        save_yolo_labels(yolo_labels, yolo_results_dir, image_name)\n",
    "        save_annotated_image( image, cv2_detections, cv2_results_dir, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_detections_for_transform(detections, image_shape):\n",
    "    \"\"\"\n",
    "    Reorders detections into srcPoints corresponding to the quadrants \n",
    "    (top-left, top-right, bottom-left, bottom-right) for homography calculation.\n",
    "\n",
    "    Args:\n",
    "        detections (list): List of dictionaries with bounding box details.\n",
    "        image_shape (tuple): Shape of the image as (height, width).\n",
    "\n",
    "    Returns:\n",
    "        list: List of four (x, y) source points in the correct order.\n",
    "    \"\"\"\n",
    "    h, w = image_shape[:2]\n",
    "\n",
    "    # Initialize lists for each quadrant\n",
    "    top_left, top_right, bottom_left, bottom_right = [], [], [], []\n",
    "\n",
    "    tl_point = (-1,-1)\n",
    "    br_point = (-1,-1)\n",
    "\n",
    "    for detection in detections:\n",
    "        x_min = detection['x_min']\n",
    "        y_min = detection['y_min']\n",
    "        x_max = detection['x_max']\n",
    "        y_max = detection['y_max']\n",
    "        center_x = (x_min + x_max) / 2\n",
    "        center_y = (y_min + y_max) / 2\n",
    "\n",
    "        # Assign to quadrants\n",
    "        if center_x < w / 2 and center_y < h / 2:\n",
    "            top_left.append((center_x, center_y))\n",
    "            tl_point = (x_min, y_min)\n",
    "        elif center_x >= w / 2 and center_y < h / 2:\n",
    "            top_right.append((center_x, center_y))\n",
    "        elif center_x < w / 2 and center_y >= h / 2:\n",
    "            bottom_left.append((center_x, center_y))\n",
    "        else:\n",
    "            bottom_right.append((center_x, center_y))\n",
    "            br_point = (x_max, y_max)\n",
    "\n",
    "    # Verify we have one point per quadrant\n",
    "    if len(top_left) != 1 or len(top_right) != 1 or len(bottom_left) != 1 or len(bottom_right) != 1:\n",
    "        raise ValueError(\"Four QR codes (one in each quadrant) are required for the transform.\")\n",
    "\n",
    "    # Return points in the order: top-left, top-right, bottom-left, bottom-right\n",
    "    out_points = [top_left[0], top_right[0], bottom_right[0], bottom_left[0]]\n",
    "    return out_points, tl_point, br_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2024-12-12 Python-3.11.11 torch-2.5.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1760518 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/targets/output/scoring_target_qr_5_h.jpg\n",
      "[{'x_min': 70.0, 'y_min': 70.0, 'x_max': 579.0, 'y_max': 579.0, 'label': 'QR 0'}, {'x_min': 2713.0, 'y_min': 63.0, 'x_max': 3236.0, 'y_max': 586.0, 'label': 'QR 1'}, {'x_min': 77.89289, 'y_min': 1978.0, 'x_max': 571.0, 'y_max': 2473.1882, 'label': 'QR 2'}, {'x_min': 2719.0, 'y_min': 1970.0, 'x_max': 3229.0, 'y_max': 2479.0, 'label': 'QR 3'}]\n",
      "{'CPU_Usage (%)': 215.9, 'Memory_Usage (bytes)': -16646144, 'Duration (seconds)': 0.7085000420047436}\n",
      "Saved annotated image: ../data/targets/scanned/cv2//images/scoring_target_qr_5_h_result.jpg\n",
      "[]\n",
      "{'CPU_Usage (%)': 466.2, 'Memory_Usage (bytes)': 43974656, 'Duration (seconds)': 0.06255654199048877}\n",
      "Saved annotated image: ../data/targets/scanned/yolo//images/scoring_target_qr_5_h_result.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../data/targets/output/scoring_target_qr_5_h.jpg\"\n",
    "# Load the model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='../models/yolov5n_best.pt')\n",
    "\n",
    "image_name = os.path.splitext(os.path.basename(filepath))[0]\n",
    "print(f\"Processing: {filepath}\")\n",
    "image = cv2.imread(filepath)\n",
    "if image is None:\n",
    "    raise ValueError(f\"Error: Could not load image at {filepath}\")\n",
    "\n",
    "\n",
    "result, stats = measure_processing_power(qr_code_detector, image)\n",
    "cv2_detections, cv2_labels = result\n",
    "print(cv2_detections)\n",
    "print(stats)\n",
    "save_annotated_image( image, cv2_detections, \"../data/targets/sandbox_run/cv2/\", image_name)\n",
    "result, stats = measure_processing_power(yolov5n_detector, image, model)\n",
    "yolo_detections, yolo_labels = result\n",
    "print(yolo_detections)\n",
    "print(stats)\n",
    "save_annotated_image( image, yolo_detections, \"../data/targets/sandbox_run/yolo/\", image_name)\n",
    "\n",
    "dst_points, _, _ = format_detections_for_transform(cv2_detections, image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_four_point_transform(srcPoints, dstPoints):\n",
    "    \"\"\"Solves for and returns a perspective transform.\n",
    "\n",
    "    Each source and corresponding destination point must be at the\n",
    "    same index in the lists.\n",
    "\n",
    "    Do not use the following functions (you will implement this yourself):\n",
    "        cv2.findHomography\n",
    "        cv2.getPerspectiveTransform\n",
    "    Hint: You will probably need to use least squares to solve this.\n",
    "    Args:\n",
    "        srcPoints (list): List of four (x,y) source points\n",
    "        dstPoints (list): List of four (x,y) destination points\n",
    "    Returns:\n",
    "        numpy.array: 3 by 3 homography matrix of floating point values\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate pair of equations for each point\n",
    "    p = srcPoints\n",
    "    t = dstPoints\n",
    "    \n",
    "    A = np.zeros((8, 9))\n",
    "    \n",
    "    for i in range(len(p)):\n",
    "        A[2 * i + 0] = [p[i][0], p[i][1], 1, 0, 0, 0, -t[i][0] * p[i][0], -t[i][0] * p[i][1], -t[i][0]]\n",
    "        A[2 * i + 1] = [0, 0, 0, p[i][0], p[i][1], 1,  -t[i][1] * p[i][0], -t[i][1] * p[i][1], -t[i][1]]\n",
    "    \n",
    "    # Execute singular value decomposition\n",
    "    u, s, vh = np.linalg.svd(A)\n",
    "    \n",
    "    # Minimizing by taking last column of V, generating homography\n",
    "    homography = np.ones((3,3))\n",
    "    count = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            homography[i,j] = vh[8,count]\n",
    "            count += 1\n",
    "    return homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescan_image(image, cv2_detections, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Rescan an image for QR codes based on approximate locations from initial detections.\n",
    "    \n",
    "    Args:\n",
    "        image (ndarray): The input image.\n",
    "        cv2_detections (list): List of initial detections with bounding box coordinates.\n",
    "        threshold (float): Threshold to determine if a QR code is within the same area.\n",
    "            A lower value means stricter matching.\n",
    "    \n",
    "    Returns:\n",
    "        rescanned_detections (list): List of valid detections that were successfully rescanned.\n",
    "    \"\"\"\n",
    "    rescanned_detections = []\n",
    "    qr_detector = cv2.QRCodeDetector()\n",
    "\n",
    "    for detection in cv2_detections:\n",
    "        # Extract and pad the bounding box\n",
    "        x_min = detection['x_min']\n",
    "        y_min = detection['y_min']\n",
    "        x_max = detection['x_max']\n",
    "        y_max = detection['y_max']\n",
    "\n",
    "        side_length = max(x_max - x_min, y_max - y_min)\n",
    "        pad = side_length * 3\n",
    "\n",
    "        x_min_padded = max(0, int(x_min - pad))\n",
    "        y_min_padded = max(0, int(y_min - pad))\n",
    "        x_max_padded = min(image.shape[1], int(x_max + pad))\n",
    "        y_max_padded = min(image.shape[0], int(y_max + pad))\n",
    "\n",
    "        # Extract the region of interest\n",
    "        roi = image[y_min_padded:y_max_padded, x_min_padded:x_max_padded]\n",
    "\n",
    "        # Rescan the region for QR codes\n",
    "        data, _, vertices, _ = qr_detector.detectAndDecodeMulti(roi)\n",
    "\n",
    "        if vertices is not None:\n",
    "            for i, points in enumerate(vertices):\n",
    "                # Convert local coordinates back to global\n",
    "                points_global = points + [x_min_padded, y_min_padded]\n",
    "                x_min_global = points_global[:, 0].min()\n",
    "                y_min_global = points_global[:, 1].min()\n",
    "                x_max_global = points_global[:, 0].max()\n",
    "                y_max_global = points_global[:, 1].max()\n",
    "\n",
    "                # Calculate overlap with the original detection\n",
    "                overlap_x = max(0, min(x_max, x_max_global) - max(x_min, x_min_global))\n",
    "                overlap_y = max(0, min(y_max, y_max_global) - max(y_min, y_min_global))\n",
    "                overlap_area = overlap_x * overlap_y\n",
    "                original_area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "                # Check if the rescan is close enough to the original detection\n",
    "                if overlap_area / original_area >= threshold:\n",
    "                    rescanned_detections.append({\n",
    "                        'x_min': x_min_global,\n",
    "                        'y_min': y_min_global,\n",
    "                        'x_max': x_max_global,\n",
    "                        'y_max': y_max_global,\n",
    "                        'label': f\"QR {i}\"\n",
    "                    })\n",
    "\n",
    "    return rescanned_detections\n",
    "\n",
    "def validate_and_redetect(image, detections, template, pyramid_levels=3, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Validate initial detections and attempt redetection if validation fails.\n",
    "\n",
    "    Args:\n",
    "        image (ndarray): The input image.\n",
    "        detections (list): List of initial detections with bounding box coordinates.\n",
    "        template (ndarray): Template of the QR code to search for.\n",
    "        pyramid_levels (int): Number of pyramid levels for validation and redetection.\n",
    "        threshold (float): Minimum confidence for template matching.\n",
    "\n",
    "    Returns:\n",
    "        updated_detections (list): List of validated or redetected bounding boxes.\n",
    "    \"\"\"\n",
    "    updated_detections = []\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Create Laplacian pyramid for the image\n",
    "    image_pyramid = [image]\n",
    "    for _ in range(pyramid_levels - 1):\n",
    "        image_pyramid.append(cv2.pyrDown(image_pyramid[-1]))\n",
    "\n",
    "    for detection in detections:\n",
    "        # Extract ROI around the detection\n",
    "        x_min, y_min, x_max, y_max = (\n",
    "            int(detection['x_min']),\n",
    "            int(detection['y_min']),\n",
    "            int(detection['x_max']),\n",
    "            int(detection['y_max']),\n",
    "        )\n",
    "\n",
    "        roi = image[y_min:y_max, x_min:x_max]\n",
    "        resized_template = cv2.resize(template, (roi.shape[1], roi.shape[0]))\n",
    "\n",
    "        # Validate using template matching within the ROI\n",
    "        result = cv2.matchTemplate(roi, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "        print(\"match result: \",cv2.minMaxLoc(result))\n",
    "\n",
    "        if max_val >= threshold:\n",
    "            # If validation succeeds, adjust coordinates to the original scale\n",
    "            updated_detections.append({\n",
    "                'x_min': x_min,\n",
    "                'y_min': y_min,\n",
    "                'x_max': x_max,\n",
    "                'y_max': y_max,\n",
    "                'confidence': max_val\n",
    "            })\n",
    "\n",
    "    return updated_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/aq_home/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2024-12-12 Python-3.11.11 torch-2.5.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1760518 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/test/samples/test_marks.JPG\n",
      "[{'x_min': 3317.0, 'y_min': 2600.667, 'x_max': 3557.0, 'y_max': 2894.2947, 'label': 'QR 0'}, {'x_min': 3360.4148, 'y_min': 1623.0, 'x_max': 3615.5322, 'y_max': 1899.5979, 'label': 'QR 1'}, {'x_min': 2364.7778, 'y_min': 1623.0138, 'x_max': 2547.1968, 'y_max': 1855.8821, 'label': 'QR 2'}, {'x_min': 1993.0, 'y_min': 21.0, 'x_max': 5709.0, 'y_max': 1544.0, 'label': 'QR 3'}, {'x_min': 2328.4646, 'y_min': 2476.3237, 'x_max': 2507.452, 'y_max': 2722.0, 'label': 'QR 4'}]\n",
      "{'CPU_Usage (%)': 116.3, 'Memory_Usage (bytes)': 208715776, 'Duration (seconds)': 2.850655124988407}\n",
      "Saved annotated image: ../data/targets/scanned/cv2/images/test_marks_result.jpg\n",
      "match result:  (0.09911054372787476, 0.09911054372787476, (0, 0), (0, 0))\n",
      "match result:  (0.10503210872411728, 0.10503210872411728, (0, 0), (0, 0))\n",
      "match result:  (0.013012969866394997, 0.013012969866394997, (0, 0), (0, 0))\n",
      "match result:  (-0.14484745264053345, -0.14484745264053345, (0, 0), (0, 0))\n",
      "match result:  (0.14061689376831055, 0.14061689376831055, (0, 0), (0, 0))\n",
      "{'CPU_Usage (%)': 99.3, 'Memory_Usage (bytes)': 1261568, 'Duration (seconds)': 0.39600608299952}\n",
      "Saved annotated image: ../data/targets/scanned/cv2/images/test_marks_result_tmatch.jpg\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../data/test/samples/test_marks.JPG\"\n",
    "# Load the model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='../models/yolov5n_best.pt')\n",
    "\n",
    "image_name = os.path.splitext(os.path.basename(filepath))[0]\n",
    "print(f\"Processing: {filepath}\")\n",
    "image = cv2.imread(filepath)\n",
    "if image is None:\n",
    "    raise ValueError(f\"Error: Could not load image at {filepath}\")\n",
    "\n",
    "\n",
    "result, stats = measure_processing_power(qr_code_detector, image)\n",
    "cv2_detections, cv2_labels = result\n",
    "print(cv2_detections)\n",
    "print(stats)\n",
    "save_annotated_image( image, cv2_detections, \"../data/targets/sandbox_run/cv2\", image_name)\n",
    "\n",
    "#print(rescan_image(image, cv2_detections, threshold=0.1))\n",
    "template = cv2.imread(\"../data/qr_codes/qr_tr.png\")\n",
    "result, stats = measure_processing_power(validate_and_redetect, image, cv2_detections, template, pyramid_levels=3, threshold=0.0)\n",
    "print(stats)\n",
    "filtered_detections = result.copy()\n",
    "save_annotated_image(image, filtered_detections, \"../data/tests/sandbox_run/cv2\", image_name, suffix=\"_tmatch\")\n",
    "\n",
    "src_points, tl_point, br_point = format_detections_for_transform(filtered_detections, image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography = find_four_point_transform(src_points, dst_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correspondences(x,y,eig):\n",
    "    \n",
    "    num_x = eig[0][0] * x + eig[0][1] * y + eig[0][2]\n",
    "    den_x = eig[2][0] * x + eig[2][1] * y + eig[2][2]\n",
    "    \n",
    "    num_y = eig[1][0] * x + eig[1][1] * y + eig[1][2]\n",
    "    den_y = eig[2][0] * x + eig[2][1] * y + eig[2][2]\n",
    "    \n",
    "    x_t = num_x / den_x\n",
    "    y_t = num_y / den_y\n",
    "    \n",
    "    return x_t, y_t\n",
    "\n",
    "def crop_to_corners(image, tl_point, br_point, eig):\n",
    "\n",
    "\n",
    "    # transform corners\n",
    "    x_tl, y_tl = correspondences(tl_point[0], tl_point[1], eig)\n",
    "    x_br, y_br = correspondences(br_point[0], br_point[1], eig)\n",
    "    x_tl, y_tl = int(x_tl), int(y_tl)\n",
    "    x_br, y_br = int(x_br), int(y_br)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image[y_tl:y_br, x_tl:x_br]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def crop_to_qr_corners(image, detections, eig):\n",
    "    \"\"\"\n",
    "    Crops the image from the top-left QR code corner to the bottom-right QR code corner.\n",
    "\n",
    "    Args:\n",
    "        image (ndarray): The input image to be cropped.\n",
    "        detections (list): List of dictionaries with bounding box details for QR codes.\n",
    "        eig (ndarray): Homography matrix.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Cropped image.\n",
    "    \"\"\"\n",
    "    # Ensure there are at least two QR codes detected\n",
    "    if len(detections) < 2:\n",
    "        raise ValueError(\"At least two QR codes (top-left and bottom-right) are required to crop the image.\")\n",
    "\n",
    "    # Find the top-left and bottom-right QR codes\n",
    "    src_points = format_detections_for_transform(detections, image.shape)\n",
    "    top_left = src_points[0].astype(int)  # Top-left QR code\n",
    "    bottom_right = src_points[2].astype(int)  # Bottom-right QR code\n",
    "\n",
    "    # Transform the top-left and bottom-right points using the homography matrix\n",
    "    x_tl, y_tl = correspondences(top_left[0], top_left[1], eig)\n",
    "    x_br, y_br = correspondences(bottom_right[0], bottom_right[1], eig)\n",
    "\n",
    "    # Ensure the points are integers for cropping\n",
    "    pad = 200\n",
    "    x_tl, y_tl = int(x_tl), int(y_tl)\n",
    "    x_br, y_br = int(x_br), int(y_br)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image[y_tl:y_br, x_tl:x_br]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped = cv2.warpPerspective(image, homography, (image.shape[1],image.shape[0]))\n",
    "warped_and_cropped = crop_to_corners(warped, tl_point, br_point, homography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size = (110,85)\n",
    "cv2.imwrite(\"../data/tests/sandbox_run/cv2/images/output_warped_cropped.jpg\",warped_and_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
